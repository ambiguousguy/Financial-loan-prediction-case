{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport scipy\nimport datetime\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-03-29T03:40:28.830678Z","iopub.execute_input":"2022-03-29T03:40:28.831661Z","iopub.status.idle":"2022-03-29T03:40:30.248502Z","shell.execute_reply.started":"2022-03-29T03:40:28.831558Z","shell.execute_reply":"2022-03-29T03:40:30.247869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %config Completer.use_jedi = False","metadata":{"execution":{"iopub.status.busy":"2022-03-29T03:40:30.249768Z","iopub.execute_input":"2022-03-29T03:40:30.250098Z","iopub.status.idle":"2022-03-29T03:40:30.254216Z","shell.execute_reply.started":"2022-03-29T03:40:30.250071Z","shell.execute_reply":"2022-03-29T03:40:30.253145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('../input/loan-model/data_clean2.csv') \ndf=df.drop(columns='Unnamed: 0')","metadata":{"execution":{"iopub.status.busy":"2022-03-29T03:40:30.255606Z","iopub.execute_input":"2022-03-29T03:40:30.256098Z","iopub.status.idle":"2022-03-29T03:40:51.223047Z","shell.execute_reply.started":"2022-03-29T03:40:30.256058Z","shell.execute_reply":"2022-03-29T03:40:51.222023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.dropna()\ndf=df.reset_index(drop=True)\n\ndata = df.drop(columns='isDefault')\ntarget=df['isDefault']\n\ndata=data.drop(columns=['annualIncome/employmentLength', 'annualIncome*employmentLength'])","metadata":{"execution":{"iopub.status.busy":"2022-03-29T03:40:51.224661Z","iopub.execute_input":"2022-03-29T03:40:51.224929Z","iopub.status.idle":"2022-03-29T03:40:53.263027Z","shell.execute_reply.started":"2022-03-29T03:40:51.2249Z","shell.execute_reply":"2022-03-29T03:40:53.262096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n# 5折交叉验证\nfolds = 5\nseed = 1\nkf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n\n\n# 分割数据集\nX_train_split, X_val, y_train_split, y_val = train_test_split(data, target, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T03:40:53.264385Z","iopub.execute_input":"2022-03-29T03:40:53.264652Z","iopub.status.idle":"2022-03-29T03:40:56.22889Z","shell.execute_reply.started":"2022-03-29T03:40:53.264621Z","shell.execute_reply":"2022-03-29T03:40:56.227839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##  随机抽取10万个样本进行测算  减少预测时间\ndata1=df.sample(n=100000,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:00:59.804802Z","iopub.execute_input":"2022-03-28T10:00:59.805128Z","iopub.status.idle":"2022-03-28T10:01:00.117571Z","shell.execute_reply.started":"2022-03-28T10:00:59.805085Z","shell.execute_reply":"2022-03-28T10:01:00.116745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1=data1.drop(columns=['annualIncome/employmentLength', 'annualIncome*employmentLength'])\ntarget1=data1['isDefault']\ndata1 = data1.drop(columns='isDefault')","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:01:00.118797Z","iopub.execute_input":"2022-03-28T10:01:00.119052Z","iopub.status.idle":"2022-03-28T10:01:00.226772Z","shell.execute_reply.started":"2022-03-28T10:01:00.119022Z","shell.execute_reply":"2022-03-28T10:01:00.225837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# params = {\n# 'booster': 'gbtree',\n# 'objective': 'binary:logistic',\n# 'metric':'auc',\n# 'eval_metric':'auc',\n# 'max_depth':5,\n# 'min_child_weight':350,\n# 'gamma':0,\n# 'subsample':1,\n# 'colsample_bytree':1,\n# 'scale_pos_weight':3,      ##  不均衡数据调整   对正样本提高损失函数的权重 \n# }\n# dtrain=xgb.DMatrix(X_train_split,y_train_split)\n\n# model= xgb.train(params=params, dtrain=dtrain)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:54.487466Z","iopub.execute_input":"2022-03-13T06:30:54.487852Z","iopub.status.idle":"2022-03-13T06:30:54.498044Z","shell.execute_reply.started":"2022-03-13T06:30:54.487816Z","shell.execute_reply":"2022-03-13T06:30:54.497257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dval=xgb.DMatrix(X_val)\n# val_pred = model.predict(dval)\n    \n# roc_auc_score(y_val, val_pred)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:54.499227Z","iopub.execute_input":"2022-03-13T06:30:54.499433Z","iopub.status.idle":"2022-03-13T06:30:54.507741Z","shell.execute_reply.started":"2022-03-13T06:30:54.499408Z","shell.execute_reply":"2022-03-13T06:30:54.507126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred=model.predict(dval)\n\n# for i in range(len(pred)):\n#     if pred[i]>0.45:\n#         pred[i]=1\n#     else:\n#         pred[i]=0\n# roc_auc_score(y_val,pred)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:54.509814Z","iopub.execute_input":"2022-03-13T06:30:54.510118Z","iopub.status.idle":"2022-03-13T06:30:54.522181Z","shell.execute_reply.started":"2022-03-13T06:30:54.510078Z","shell.execute_reply":"2022-03-13T06:30:54.521377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# roc_auc_score(y_val, pred)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:54.524708Z","iopub.execute_input":"2022-03-13T06:30:54.525434Z","iopub.status.idle":"2022-03-13T06:30:54.531407Z","shell.execute_reply.started":"2022-03-13T06:30:54.525399Z","shell.execute_reply":"2022-03-13T06:30:54.530865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clf = XGBClassifier(\n#         #树的个数\n#         n_estimators=100,\n#         # 如同学习率\n#         learning_rate= 0.3, \n#         # 构建树的深度，越大越容易过拟合    \n#         max_depth=6, \n#         # 随机采样训练样本 训练实例的子采样比\n#         subsample=1, \n#         # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子\n#         gamma=0, \n#         # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n#         reg_lambda=1,  \n        \n#         #最大增量步长，我们允许每个树的权重估计。\n#         max_delta_step=0,\n#         # 生成树时进行的列采样 \n#         colsample_bytree=1, \n\n#         # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n#         # 假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n#         #这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。\n#         min_child_weight=1, \n\n#         #随机种子,\n#         seed=1000 ,\n        \n#         # L1 正则项参数\n# #        reg_alpha=0,\n        \n#         #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。平衡正负权重\n#         scale_pos_weight=3,          ##########\n        \n#         #多分类的问题 指定学习任务和相应的学习目标\n#         #objective= 'multi:softmax', \n        \n#         # 类别数，多分类与 multisoftmax 并用\n#         #num_class=10,\n        \n#         # 设置成1则没有运行信息输出，最好是设置为0.是否在运行升级时打印消息。\n# #        silent=0 ,\n#         # cpu 线程数 默认最大\n# #        nthread=4,\n    \n#         #eval_metric= 'auc'\n# )\n\n\n# clf.fit(X_train_split,y_train_split,eval_metric= 'auc')\n\n# pred=clf.predict(X_val)\n\n# roc_auc_score(y_val,pred)\n\n# ########################################################################\n# 在xgbclassifier 调整了对正样本的损失函数权重后  scale_pos_weight=3  AUC 从0.53 提高到了 0.651\n","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:54.532651Z","iopub.execute_input":"2022-03-13T06:30:54.533128Z","iopub.status.idle":"2022-03-13T06:30:54.543271Z","shell.execute_reply.started":"2022-03-13T06:30:54.533099Z","shell.execute_reply":"2022-03-13T06:30:54.5427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"####  关于在xgbclassifier 添加参数     好像不能直接使用字典赋值  以下：\n\nparams = {\n    'booster':'gbtree',\n    'objective':'multi:softmax',\n    'num_class':3,\n    'gamma':0.1,\n    'max_depth':6,\n    'lambda':2,\n    'subsample':0.7,\n    'colsample_bytree':0.7,\n    'min_child_weight':3,\n    'slient':1,\n    'eta':0.1,\n    'seed':1000,\n    'nthread':4,\n}\n \nplst = params.items()               ##  然后再把生成的plst 置于fit()函数中进行识别","metadata":{"execution":{"iopub.status.busy":"2022-03-10T15:17:34.281597Z","iopub.execute_input":"2022-03-10T15:17:34.281917Z","iopub.status.idle":"2022-03-10T15:17:34.853016Z","shell.execute_reply.started":"2022-03-10T15:17:34.281885Z","shell.execute_reply":"2022-03-10T15:17:34.852123Z"}}},{"cell_type":"code","source":"# params = { 'booster':'gbtree', 'objective':'multi:softmax', 'num_class':3, 'gamma':0.1, 'max_depth':6, 'lambda':2, 'subsample':0.7, 'colsample_bytree':0.7, 'min_child_weight':3, 'slient':1, 'eta':0.1, 'seed':1000, 'nthread':4, }\n\n# plst = params.items() \n# plst","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:54.544911Z","iopub.execute_input":"2022-03-13T06:30:54.545202Z","iopub.status.idle":"2022-03-13T06:30:54.558211Z","shell.execute_reply.started":"2022-03-13T06:30:54.545165Z","shell.execute_reply":"2022-03-13T06:30:54.557422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 尝试对xgbclassifier 进行调参看看能否有较为明显的提升","metadata":{}},{"cell_type":"markdown","source":"##  调参方向\n首先调整**max_depth** ,通常max_depth 这个参数与其他参数关系不大，初始值设置为10，找到一个最好的误差值，然后就可以调整参数与这个误差值进行对比。比如调整到8，如果此时最好的误差变高了，那么下次就调整到12；如果调整到12,误差值比10 的低，那么下次可以尝试调整到15.\n在找到了最优的max_depth之后，可以开始调整**subsample**,初始值设置为1，然后调整到0.8 如果误差值变高，下次就调整到0.9，如果还是变高，就保持为1.0\n接着开始调整**min_child_weight** , 方法与上面同理\n再接着调整**colsample_bytree**\n经过上面的调整，已经得到了一组参数，这时调整eta 到0.05，然后让程序运行来得到一个最佳的num_round,(在 误差值开始上升趋势的时候为最佳 )\n","metadata":{}},{"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n\n# def get_best_cv_params(learning_rate=0.3,max_depth=6,subsample=1,min_child_weight=1,colsample_bytree=1,scale_pos_weight=3,n_estimators=100,param_grid=None):\n#     #     # 设置5折交叉验证\n#     kf = KFold(n_splits=5, shuffle=True, random_state=0)\n    \n#     model_xgb= XGBClassifier(\n#                                 learning_rate=learning_rate,\n#                                 max_depth=max_depth,\n#                                 subsample=subsample,\n#                                 min_child_weight=min_child_weight,\n#                                 colsample_bytree=colsample_bytree,\n#                                 scale_pos_weight=scale_pos_weight,\n#                                 n_estimators=n_estimators,\n#                                 eval_metric='error'\n#                                   )\n#     grid_search = GridSearchCV(estimator=model_xgb, \n#                                cv=kf,\n#                                param_grid=param_grid,\n#                                scoring='roc_auc'\n#                               )\n#     grid_search.fit(data1, target1)\n\n#     print('模型当前最优参数为:{}'.format(grid_search.best_params_))\n#     print('模型当前最优得分为:{}'.format(grid_search.best_score_))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:54.561509Z","iopub.execute_input":"2022-03-13T06:30:54.562195Z","iopub.status.idle":"2022-03-13T06:30:54.568631Z","shell.execute_reply.started":"2022-03-13T06:30:54.562158Z","shell.execute_reply":"2022-03-13T06:30:54.568032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # max_depth   min_child _weight  调参\n# xgb_params = {'max_depth': range(3,10,2),'min_child_weight':range(1,6,2)}\n# get_best_cv_params(learning_rate=0.3,max_depth=None,subsample=1,min_child_weight=None,colsample_bytree=1,param_grid=xgb_params)\n\n\n# # 模型当前最优参数为:{'max_depth': 3, 'min_child_weight': 5}\n# # 模型当前最优得分为:0.7202592677725501","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:54.569504Z","iopub.execute_input":"2022-03-13T06:30:54.570163Z","iopub.status.idle":"2022-03-13T06:30:54.583307Z","shell.execute_reply.started":"2022-03-13T06:30:54.570132Z","shell.execute_reply":"2022-03-13T06:30:54.582758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 由于XGBOOST classifier 的表现确实不佳， 只能转而先试试lgbt算法\n\n# lgbt算法","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:19:10.741217Z","iopub.execute_input":"2022-03-28T10:19:10.741543Z","iopub.status.idle":"2022-03-28T10:19:10.745912Z","shell.execute_reply.started":"2022-03-28T10:19:10.741508Z","shell.execute_reply":"2022-03-28T10:19:10.745009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import lightgbm as lgb\n# # 数据集划分\n# X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2)\n# train_matrix = lgb.Dataset(X_train_split, label=y_train_split)\n# valid_matrix = lgb.Dataset(X_val, label=y_val)\n\n# params = {\n#             'boosting_type': 'gbdt',\n#             'objective': 'binary',\n#             'learning_rate': 0.1,\n#             'metric': 'auc',\n#             'min_child_weight': 1e-3,\n#             'num_leaves': 31,\n#             'max_depth': -1,\n#             'reg_lambda': 0,\n#             'reg_alpha': 0,\n#             'feature_fraction': 1,\n#             'bagging_fraction': 1,\n#             'bagging_freq': 0,\n#             'seed': 2020,\n#             'nthread': 8,\n#             'silent': True,\n#             'verbose': -1,\n# }\n\n# \"\"\"使用训练集数据进行模型训练\"\"\"\n# model = lgb.train(params, train_set=train_matrix, valid_sets=valid_matrix, num_boost_round=20000, verbose_eval=1000, early_stopping_rounds=200)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.472945Z","iopub.execute_input":"2022-03-13T06:30:55.473247Z","iopub.status.idle":"2022-03-13T06:30:55.478069Z","shell.execute_reply.started":"2022-03-13T06:30:55.473209Z","shell.execute_reply":"2022-03-13T06:30:55.477202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"\"\"使用lightgbm 5折交叉验证进行建模预测\"\"\"\n# cv_scores = []\n# for i, (train_index, valid_index) in enumerate(kf.split(data, target)):\n#     print('************************************ {} ************************************'.format(str(i+1)))\n#     X_train_split, y_train_split, X_val, y_val = data.iloc[train_index], target[train_index], data.iloc[valid_index], target[valid_index]\n    \n#     train_matrix = lgb.Dataset(X_train_split, label=y_train_split)\n#     valid_matrix = lgb.Dataset(X_val, label=y_val)\n\n#     params = {\n#                 'boosting_type': 'gbdt',\n#                 'objective': 'binary',\n#                 'learning_rate': 0.1,\n#                 'metric': 'auc',\n        \n#                 'min_child_weight': 1e-3,\n#                 'num_leaves': 31,\n#                 'max_depth': -1,\n#                 'reg_lambda': 0,\n#                 'reg_alpha': 0,\n#                 'feature_fraction': 1,\n#                 'bagging_fraction': 1,\n#                 'bagging_freq': 0,\n#                 'seed': 2020,\n#                 'nthread': 8,\n#                 'silent': True,\n#                 'verbose': -1,\n#     }\n    \n#     model = lgb.train(params, train_set=train_matrix, num_boost_round=20000, valid_sets=valid_matrix, verbose_eval=1000, early_stopping_rounds=200)\n#     val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n    \n#     cv_scores.append(roc_auc_score(y_val, val_pred))\n#     print(cv_scores)\n\n# print(\"lgb_scotrainre_list:{}\".format(cv_scores))\n# print(\"lgb_score_mean:{}\".format(np.mean(cv_scores)))\n# print(\"lgb_score_std:{}\".format(np.std(cv_scores)))\n\n\n\n##   the avg AUC 0.7274","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:06:35.120728Z","iopub.execute_input":"2022-03-28T10:06:35.122009Z","iopub.status.idle":"2022-03-28T10:10:08.915253Z","shell.execute_reply.started":"2022-03-28T10:06:35.121952Z","shell.execute_reply":"2022-03-28T10:10:08.914293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##   LGBregressor 平均 AUC 0.7274","metadata":{}},{"cell_type":"code","source":"# import joblib\n\n# joblib.dump(model, './lgb_default.pkl')  ## 模型保存","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:12:03.852935Z","iopub.execute_input":"2022-03-28T10:12:03.854008Z","iopub.status.idle":"2022-03-28T10:12:03.906849Z","shell.execute_reply.started":"2022-03-28T10:12:03.853959Z","shell.execute_reply":"2022-03-28T10:12:03.906129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val_pred","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.493541Z","iopub.execute_input":"2022-03-13T06:30:55.493881Z","iopub.status.idle":"2022-03-13T06:30:55.505921Z","shell.execute_reply.started":"2022-03-13T06:30:55.493838Z","shell.execute_reply":"2022-03-13T06:30:55.505118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"\"\"使用xgboost classifier  默认参数  5折交叉验证进行建模预测\"\"\"\n\n# cv_scores = []\n# for i, (train_index, valid_index) in enumerate(kf.split(data, target)):\n#     print('************************************ {} ************************************'.format(str(i+1)))\n#     X_train_split, y_train_split, X_val, y_val = data.iloc[train_index], target[train_index], data.iloc[valid_index], target[valid_index]\n    \n# #     params={'booster':'gbtree',\n# #     'objective': 'binary:logistic',\n# #     'eval_metric': 'auc',\n# #     'max_depth':4,\n# #     'lambda':10,\n# #     'subsample':0.75,\n# #     'colsample_bytree':0.75,\n# #     'min_child_weight':2,\n# #     'eta': 0.025,\n# #     'seed':0,\n# #     'nthread':8,\n# #      'silent':1}\n\n    \n    \n#     xgb = XGBClassifier()\n#     model=xgb.fit(X_train_split,y_train_split) #,early_stopping_rounds= 200) \n#     val_pred = model.predict(X_val)\n    \n#     cv_scores.append(roc_auc_score(y_val, val_pred))\n#     print(cv_scores)\n\n# print(\"xgb_scotrainre_list:{}\".format(cv_scores))\n# print(\"xgb_score_mean:{}\".format(np.mean(cv_scores)))\n# print(\"xgb_score_std:{}\".format(np.std(cv_scores)))","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.50721Z","iopub.execute_input":"2022-03-13T06:30:55.507658Z","iopub.status.idle":"2022-03-13T06:30:55.517Z","shell.execute_reply.started":"2022-03-13T06:30:55.507616Z","shell.execute_reply":"2022-03-13T06:30:55.516295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"************************************ 1 ************************************\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n[13:15:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[0.541241746781573]\n************************************ 2 ************************************\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n[13:19:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[0.541241746781573, 0.541330343692193]\n************************************ 3 ************************************\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n[13:23:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[0.541241746781573, 0.541330343692193, 0.5412088769869096]\n************************************ 4 ************************************\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n[13:27:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[0.541241746781573, 0.541330343692193, 0.5412088769869096, 0.5416327845593777]\n************************************ 5 ************************************\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n[13:32:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[0.541241746781573, 0.541330343692193, 0.5412088769869096, 0.5416327845593777, 0.5392113616293839]\nxgb_scotrainre_list:[0.541241746781573, 0.541330343692193, 0.5412088769869096, 0.5416327845593777, 0.5392113616293839]\n\n\nxgb_score_mean:0.5409250227298875\nxgb_score_std:0.0008697969442683131","metadata":{}},{"cell_type":"markdown","source":"使用 xgb classifier的效果并不好   只有平均0.5409的AUC","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 对xgboostregressor 进行测算","metadata":{}},{"cell_type":"code","source":"# \"\"\"使用xgboost regegressor  默认参数  5折交叉验证进行建模预测\"\"\"\n\n# from xgboost import XGBRegressor\n\n# cv_scores = []\n# for i, (train_index, valid_index) in enumerate(kf.split(data, target)):\n#     print('************************************ {} ************************************'.format(str(i+1)))\n#     X_train_split, y_train_split, X_val, y_val = data.iloc[train_index], target[train_index], data.iloc[valid_index], target[valid_index]\n    \n# #     params={'booster':'gbtree',\n# #     'objective': 'binary:logistic',\n# #     'eval_metric': 'auc',\n# #     'max_depth':4,\n# #     'lambda':10,\n# #     'subsample':0.75,\n# #     'colsample_bytree':0.75,\n# #     'min_child_weight':2,\n# #     'eta': 0.025,\n# #     'seed':0,\n# #     'nthread':8,\n# #      'silent':1}\n\n    \n    \n#     xgb = XGBRegressor()\n#     model=xgb.fit(X_train_split,y_train_split) #,early_stopping_rounds= 200) \n#     val_pred = model.predict(X_val)\n    \n#     cv_scores.append(roc_auc_score(y_val, val_pred))\n#     print(cv_scores)\n\n# print(\"xgb_scotrainre_list:{}\".format(cv_scores))\n# print(\"xgb_score_mean:{}\".format(np.mean(cv_scores)))\n# print(\"xgb_score_std:{}\".format(np.std(cv_scores)))","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.51836Z","iopub.execute_input":"2022-03-13T06:30:55.519217Z","iopub.status.idle":"2022-03-13T06:30:55.531958Z","shell.execute_reply.started":"2022-03-13T06:30:55.519175Z","shell.execute_reply":"2022-03-13T06:30:55.531126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#############################################################\n\n在默认参数下  xgbregressor 的平均 AUC约为 0.7245   标准差波动约为 0.00191   波动不大","metadata":{}},{"cell_type":"code","source":"'模型的保存与读取  避免重复训练模型花费大量时间'\n\nimport joblib\n\n#joblib.dump(model, './model/xgb_default.pkl')  ## 模型保存\n\n# model= joblib.load('./model/xgb_default.pkl') ##  模型读取\n# model.predict()","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.533125Z","iopub.execute_input":"2022-03-13T06:30:55.533628Z","iopub.status.idle":"2022-03-13T06:30:55.545832Z","shell.execute_reply.started":"2022-03-13T06:30:55.533593Z","shell.execute_reply":"2022-03-13T06:30:55.545192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 默认超参数：\n\n# parameter={'base_score':0.5, \n#            'booster':'gbtree', \n#            'colsample_bylevel':1,\n#              'colsample_bynode':1, \n#            'colsample_bytree':1, \n#            'enable_categorical':False,\n#              'gamma':0, \n#            'gpu_id':-1, \n#            'importance_type':None,\n#           'learning_rate':0.300000012,\n#              'max_delta_step':0,\n#            'max_depth':6, \n#            'in_child_weight':1, \n#            'missing':nan,\n#            'n_estimators':100, \n#            'n_jobs':4,\n#              'num_parallel_tree':1, \n#           'predictor':'auto', \n#            'random_state':0, \n#            'reg_alpha':0,\n#              'reg_lambda':1, \n#            'scale_pos_weight':1, \n#            'subsample':1, \n#            'tree_method':'exact',\n#              \"validate_parameters':1, \n#            'verbosity':None\n    \n# }\n\n\n\n# 'booster':'gbtree', 这个指定基分类器\n# 'objective': 'multi:softmax', 多分类的问题， 这个是优化目标，必须得有，因为xgboost里面有求一阶导数和二阶导数，其实就是这个。\n# 'num_class':10, 类别数，与 multisoftmax 并用\n# 'gamma':损失下降多少才进行分裂， 控制叶子节点的个数\n# 'max_depth':12, 构建树的深度，越大越容易过拟合\n# 'lambda':2, 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n# 'subsample':0.7, 随机采样训练样本\n# 'colsample_bytree':0.7, 生成树时进行的列采样\n# 'min_child_weight':3, 孩子节点中最小的样本权重和。如果一个叶子节点的样本权重和小于min_child_weight则拆分过程结束\n# 'silent':0 ,设置成1则没有运行信息输出，最好是设置为0.\n# 'eta': 0.007, 如同学习率，这个怎么感觉像是每棵树前面的那个权重呢？\n# 'seed':1000,\n# 'nthread':7, cpu 线程数\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.547337Z","iopub.execute_input":"2022-03-13T06:30:55.547906Z","iopub.status.idle":"2022-03-13T06:30:55.556556Z","shell.execute_reply.started":"2022-03-13T06:30:55.547864Z","shell.execute_reply":"2022-03-13T06:30:55.556001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  调参方向\n首先调整**max_depth** ,通常max_depth 这个参数与其他参数关系不大，初始值设置为10，找到一个最好的误差值，然后就可以调整参数与这个误差值进行对比。比如调整到8，如果此时最好的误差变高了，那么下次就调整到12；如果调整到12,误差值比10 的低，那么下次可以尝试调整到15.\n在找到了最优的max_depth之后，可以开始调整**subsample**,初始值设置为1，然后调整到0.8 如果误差值变高，下次就调整到0.9，如果还是变高，就保持为1.0\n接着开始调整**min_child_weight** , 方法与上面同理\n再接着调整**colsample_bytree**\n经过上面的调整，已经得到了一组参数，这时调整eta 到0.05，然后让程序运行来得到一个最佳的num_round,(在 误差值开始上升趋势的时候为最佳 )\n","metadata":{}},{"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n# import time \n\n# def get_best_cv_params(learning_rate=0.3,max_depth=6,subsample=1,min_child_weight=1,colsample_bytree=1,scale_pos_weight=3,n_estimators=100,param_grid=None):\n#     #     # 设置5折交叉验证\n    \n#     time_start=time.time()\n#     kf = KFold(n_splits=5, shuffle=True, random_state=0)\n    \n#     model_xgb= XGBRegressor(\n#                                 learning_rate=learning_rate,\n#                                 max_depth=max_depth,\n#                                 subsample=subsample,             ## 理论范围0-1   一般调参范围 0.8-1 ？  大概吧\n#                                 min_child_weight=min_child_weight,\n#                                 colsample_bytree=colsample_bytree,      ##  每个树生成的随机选择的选择的变量数 / 列数   一般范围 0.5-1\n#                                 scale_pos_weight=scale_pos_weight,       ##  由于数据呈现不均衡态   因而设置权重  在损失函数中更多的偏向正样本  （3倍）\n#                                 n_estimators=n_estimators,           ##  生成树的数量\n#                                 eval_metric='auc'\n#                                   )\n#     grid_search = GridSearchCV(estimator=model_xgb, \n#                                cv=kf,\n#                                param_grid=param_grid,\n#                                scoring='roc_auc'\n#                               )\n#     grid_search.fit(data1, target1)\n#     time_end=time.time()\n\n#     print('模型当前最优参数为:{}'.format(grid_search.best_params_))\n#     print('模型当前最优得分为:{}'.format(grid_search.best_score_))\n#     print('耗时：{}'.format(time_end-time_start))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.558163Z","iopub.execute_input":"2022-03-13T06:30:55.55869Z","iopub.status.idle":"2022-03-13T06:30:55.571214Z","shell.execute_reply.started":"2022-03-13T06:30:55.558647Z","shell.execute_reply":"2022-03-13T06:30:55.570474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # max_depth   min_child _weight  调参\n\n# xgb_params = {'max_depth': range(3,10,2),'min_child_weight':range(1,6,2)}\n# get_best_cv_params(learning_rate=0.3,max_depth=None,subsample=0.6,min_child_weight=None,colsample_bytree=0.7,scale_pos_weight=3,n_estimators=100,param_grid=xgb_params)\n\n\n\n# **************模型当前最优参数为:{'max_depth': 3, 'min_child_weight': 1}*****************************\n\n# 模型当前最优参数为:{'max_depth': 3, 'min_child_weight': 1}\n# 模型当前最优得分为:0.7167870081408407\n# 耗时：1846.435424566269","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.572647Z","iopub.execute_input":"2022-03-13T06:30:55.573036Z","iopub.status.idle":"2022-03-13T06:30:55.585787Z","shell.execute_reply.started":"2022-03-13T06:30:55.572993Z","shell.execute_reply":"2022-03-13T06:30:55.585053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_depth   min_child _weight  调参\n# xgb_params = {'max_depth': [2,3,4]}\n# get_best_cv_params(learning_rate=0.3,max_depth=None,subsample=0.6,min_child_weight=1,colsample_bytree=0.7,scale_pos_weight=3,n_estimators=100,param_grid=xgb_params)\n\n\n# ***************************模型当前最优参数为:{'max_depth': 2}***********************\n\n# 模型当前最优参数为:{'max_depth': 2}\n# 模型当前最优得分为:0.7196428084978479\n# 耗时：256.90805101394653","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.587113Z","iopub.execute_input":"2022-03-13T06:30:55.587385Z","iopub.status.idle":"2022-03-13T06:30:55.598389Z","shell.execute_reply.started":"2022-03-13T06:30:55.587347Z","shell.execute_reply":"2022-03-13T06:30:55.597564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ##  subsample  调参     默认1 \n\n# xgb_params = {'sub_sample': [0.6,0.7,0.8,0.9,1]}\n# get_best_cv_params(learning_rate=0.3,max_depth=2,subsample=None,min_child_weight=1,colsample_bytree=0.7,scale_pos_weight=3,n_estimators=100,param_grid=xgb_params)\n\n# 模型当前最优参数为:{'sub_sample': 0.6}\n# 模型当前最优得分为:0.7199071582664202\n# 耗时：264.6281690597534","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.599476Z","iopub.execute_input":"2022-03-13T06:30:55.600017Z","iopub.status.idle":"2022-03-13T06:30:55.607057Z","shell.execute_reply.started":"2022-03-13T06:30:55.599986Z","shell.execute_reply":"2022-03-13T06:30:55.606511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xgb_params = {'sub_sample': [0.4,0.45,0.5,0.55,0.6]}\n# get_best_cv_params(learning_rate=0.3,max_depth=2,subsample=None,min_child_weight=1,colsample_bytree=0.7,scale_pos_weight=3,n_estimators=100,param_grid=xgb_params)\n\n# 模型当前最优参数为:{'sub_sample': 0.4}\n# 模型当前最优得分为:0.7199071582664202\n# 耗时：264.4010627269745                               ","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.608032Z","iopub.execute_input":"2022-03-13T06:30:55.608364Z","iopub.status.idle":"2022-03-13T06:30:55.61626Z","shell.execute_reply.started":"2022-03-13T06:30:55.608337Z","shell.execute_reply":"2022-03-13T06:30:55.615528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xgb_params = {'sub_sample': [0.2,0.3,0.35,0.4]}\n# get_best_cv_params(learning_rate=0.3,max_depth=2,subsample=None,min_child_weight=1,colsample_bytree=0.7,scale_pos_weight=3,n_estimators=100,param_grid=xgb_params)\n\n# 模型当前最优参数为:{'sub_sample': 0.2}\n# 模型当前最优得分为:0.7199071582664202            ## sub_sample  子样本集还能再降低？？？？？\n# 耗时：214.398024559021","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.617231Z","iopub.execute_input":"2022-03-13T06:30:55.61752Z","iopub.status.idle":"2022-03-13T06:30:55.626339Z","shell.execute_reply.started":"2022-03-13T06:30:55.617479Z","shell.execute_reply":"2022-03-13T06:30:55.625782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xgb_params = {'sub_sample': [0.05,0.1,0.15,0.2]}\n# get_best_cv_params(learning_rate=0.3,max_depth=2,subsample=None,min_child_weight=1,colsample_bytree=0.7,scale_pos_weight=3,n_estimators=100,param_grid=xgb_params)\n\n# 模型当前最优参数为:{'sub_sample': 0.05}\n# 模型当前最优得分为:0.7199071582664202\n# 耗时：214.34074664115906","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.627817Z","iopub.execute_input":"2022-03-13T06:30:55.628393Z","iopub.status.idle":"2022-03-13T06:30:55.639182Z","shell.execute_reply.started":"2022-03-13T06:30:55.628363Z","shell.execute_reply":"2022-03-13T06:30:55.638356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"先把最优sub_sample 设定为0.6  ","metadata":{}},{"cell_type":"code","source":"#  ##   colsample_bytree 调参  默认1   调参范围  0.6-1\n    \n# xgb_params = {'colsample_bytree': [0.6,0.7,0.8,0.9,1]}\n# get_best_cv_params(learning_rate=0.3,max_depth=2,subsample=0.6,min_child_weight=1,colsample_bytree=None,scale_pos_weight=3,n_estimators=100,param_grid=xgb_params)\n\n# 模型当前最优参数为:{'colsample_bytree': 0.7}\n# 模型当前最优得分为:0.7196428084978479\n# 耗时：338.04453206062317","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.640509Z","iopub.execute_input":"2022-03-13T06:30:55.641207Z","iopub.status.idle":"2022-03-13T06:30:55.648307Z","shell.execute_reply.started":"2022-03-13T06:30:55.641171Z","shell.execute_reply":"2022-03-13T06:30:55.647641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ##   scale_pos_weight 调参  默认1   调参范围  3,4,5,6,7\n    \n# xgb_params = {'scale_pos_weight': [3,4,5,6,7]}\n# get_best_cv_params(learning_rate=0.3,max_depth=2,subsample=0.6,min_child_weight=1,colsample_bytree=0.7,scale_pos_weight=None,n_estimators=100,param_grid=xgb_params)\n\n\n# 模型当前最优参数为:{'scale_pos_weight': 3}\n# 模型当前最优得分为:0.7196428084978479\n# 耗时：308.4445559978485","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.649518Z","iopub.execute_input":"2022-03-13T06:30:55.650063Z","iopub.status.idle":"2022-03-13T06:30:55.659356Z","shell.execute_reply.started":"2022-03-13T06:30:55.65003Z","shell.execute_reply":"2022-03-13T06:30:55.658645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ##   scale_pos_weight 调参  默认1   调参范围  3,4,5,6,7\n    \n# xgb_params = {'scale_pos_weight': [0.5,1,1.5,2,2.5,3]}\n# get_best_cv_params(learning_rate=0.3,max_depth=2,subsample=0.6,min_child_weight=1,colsample_bytree=0.7,scale_pos_weight=None,n_estimators=100,param_grid=xgb_params)\n\n# 模型当前最优参数为:{'scale_pos_weight': 3}\n# 模型当前最优得分为:0.7196428084978479               或许是因为scale_pos_weight在一开始就已经设定  后续的参数都是由该变量变化   所以导致最优是3\n# 耗时：366.77433800697327\n","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.660702Z","iopub.execute_input":"2022-03-13T06:30:55.661196Z","iopub.status.idle":"2022-03-13T06:30:55.670702Z","shell.execute_reply.started":"2022-03-13T06:30:55.661154Z","shell.execute_reply":"2022-03-13T06:30:55.669922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 学习率  调整\n# xgb_params = {'learning_rate': [0.1,0.2,0.3,0.4]}\n# get_best_cv_params(learning_rate=None,max_depth=2,subsample=0.6,min_child_weight=1,colsample_bytree=0.7,scale_pos_weight=3,n_estimators=100,param_grid=xgb_params)\n\n# 模型当前最优参数为:{'learning_rate': 0.2}\n# 模型当前最优得分为:0.7198354938026809\n# 耗时：251.48691415786743","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.671844Z","iopub.execute_input":"2022-03-13T06:30:55.672065Z","iopub.status.idle":"2022-03-13T06:30:55.682204Z","shell.execute_reply.started":"2022-03-13T06:30:55.672039Z","shell.execute_reply":"2022-03-13T06:30:55.681643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##  n_estimators 调整\n# xgb_params = {'n_estimators': [100,150,200,300]}\n# get_best_cv_params(learning_rate=0.2,max_depth=2,subsample=0.6,min_child_weight=1,colsample_bytree=0.7,scale_pos_weight=3,n_estimators=None,param_grid=xgb_params)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.683587Z","iopub.execute_input":"2022-03-13T06:30:55.684297Z","iopub.status.idle":"2022-03-13T06:30:55.691622Z","shell.execute_reply.started":"2022-03-13T06:30:55.684255Z","shell.execute_reply":"2022-03-13T06:30:55.691095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 最优参数 ：  learning_rate=0.2,max_depth=2,subsample=0.6,min_child_weight=1,colsample_bytree=0.7,scale_pos_weight=3,n_estimators=200,param_grid=xgb_params","metadata":{}},{"cell_type":"code","source":"# cv_scores = []\n# for i, (train_index, valid_index) in enumerate(kf.split(data, target)):\n#     print('************************************ {} ************************************'.format(str(i+1)))\n#     X_train_split, y_train_split, X_val, y_val = data.iloc[train_index], target[train_index], data.iloc[valid_index], target[valid_index]\n    \n# #     params = {\n# #                 'learning_rate':0.1,\n# #                 'max_depth':2,\n# #                 'subsample':0.6,\n# #                 'min_child_weight':1,\n# #                 'colsample_bytree':0.7,\n# #                 'scale_pos_weight':3,\n# #                 'n_estimators':200\n# #                }\n\n    \n    \n#     xgb = XGBRegressor(learning_rate=0.2,max_depth=2,subsample=0.6,min_child_weight=1,colsample_bytree=0.7,scale_pos_weight=3,n_estimators=200)\n#     model=xgb.fit(X_train_split,y_train_split) #,early_stopping_rounds= 200) \n#     val_pred = model.predict(X_val)\n    \n#     cv_scores.append(roc_auc_score(y_val, val_pred))\n#     print(cv_scores)\n\n# print(\"xgb_scotrainre_list:{}\".format(cv_scores))\n# print(\"xgb_score_mean:{}\".format(np.mean(cv_scores)))\n# print(\"xgb_score_std:{}\".format(np.std(cv_scores)))\n\n\n\n# xgb_scotrainre_list:[0.7229786755216663, 0.7214270436404034, 0.7267020050178964, 0.7256152188581936, 0.7237018527949425]\n# xgb_score_mean:0.7240849591666204\n# xgb_score_std:0.0018775394883172429\n\n\n# ***************************平均 AUC 0.72408*******************************************\n\n#  基于百分之十数据生成的子集并没有使总体的数据模型的AUC有明显的提升    下一步只能使用全集来进行参数调整    但是时间会消耗很多","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.692982Z","iopub.execute_input":"2022-03-13T06:30:55.693393Z","iopub.status.idle":"2022-03-13T06:30:55.702163Z","shell.execute_reply.started":"2022-03-13T06:30:55.693307Z","shell.execute_reply":"2022-03-13T06:30:55.701489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 对全部训练数据放进模型进行调参   看看是否有优化","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nimport time \n\ndef get_best_cv_params(learning_rate=0.3,max_depth=6,subsample=1,min_child_weight=1,colsample_bytree=1,scale_pos_weight=3,n_estimators=100,param_grid=None):\n    #     # 设置5折交叉验证\n    \n    time_start=time.time()\n    kf = KFold(n_splits=5, shuffle=True, random_state=0)\n    \n    model_xgb= XGBRegressor(\n                                learning_rate=learning_rate,\n                                max_depth=max_depth,\n                                subsample=subsample,             ## 理论范围0-1   一般调参范围 0.8-1 ？  大概吧\n                                min_child_weight=min_child_weight,\n                                colsample_bytree=colsample_bytree,      ##  每个树生成的随机选择的选择的变量数 / 列数   一般范围 0.5-1\n                                scale_pos_weight=scale_pos_weight,       ##  由于数据呈现不均衡态   因而设置权重  在损失函数中更多的偏向正样本  （3倍）\n                                n_estimators=n_estimators,           ##  生成树的数量\n                                eval_metric='auc'\n                                  )\n    grid_search = GridSearchCV(estimator=model_xgb, \n                               cv=kf,\n                               param_grid=param_grid,\n                               scoring='roc_auc'\n                              )\n    grid_search.fit(data, target)\n    time_end=time.time()\n\n    print('模型当前最优参数为:{}'.format(grid_search.best_params_))\n    print('模型当前最优得分为:{}'.format(grid_search.best_score_))\n    print('耗时：{}'.format(time_end-time_start))","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.703697Z","iopub.execute_input":"2022-03-13T06:30:55.704239Z","iopub.status.idle":"2022-03-13T06:30:55.716618Z","shell.execute_reply.started":"2022-03-13T06:30:55.704196Z","shell.execute_reply":"2022-03-13T06:30:55.716045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # max_depth   min_child _weight  调参\n\nxgb_params = {'max_depth': range(3,10,2),'min_child_weight':range(1,6,2)}\nget_best_cv_params(learning_rate=0.3,max_depth=None,subsample=0.6,min_child_weight=None,colsample_bytree=0.7,scale_pos_weight=3,n_estimators=100,param_grid=xgb_params)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:30:55.717463Z","iopub.execute_input":"2022-03-13T06:30:55.718161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"\"\"\n# 参数确定好了以后，我们设置一个比较小的learning_rate 0.005，来确定最终的num_boost_round\n# \"\"\"\n# # 设置5折交叉验证\n# # cv_fold = StratifiedKFold(n_splits=5, random_state=0, shuffle=True, )\n# final_params = {\n#                 'learning_rate':0.1,\n#                 'max_depth':2,\n#                 'subsample':0.6,\n#                 'min_child_weight':1,\n#                 'colsample_bytree':0.7,\n#                 'scale_pos_weight':3,\n#                 'n_estimators':200\n#                }\n\n\n# xgb_train1=xgb.DMatrix(data1,target1)\n\n# cv_result = xgb.cv(dtrain=xgb_train1,\n#                    early_stopping_rounds=20,\n#                    num_boost_round=5000,\n#                    nfold=5,\n#                    stratified=True,\n#                    shuffle=True,\n#                    params=final_params,\n#                    metrics='auc',\n#                    seed=0,\n#                   )\n\n# print('迭代次数{}'.format(len(cv_result)))\n# print('交叉验证的AUC为{}'.format(max(cv_result['test-auc-mean'])))\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n\n# def get_best_cv_params(learning_rate=0.3,max_depth=6,subsample=1,min_child_weight=1,colsample_bytree=1,scale_pos_weight=3,n_estimators=100,param_grid=None):\n#     #     # 设置5折交叉验证\n#     kf = KFold(n_splits=5, shuffle=True, random_state=0)\n    \n#     model_xgb= XGBClassifier(\n#                                 learning_rate=learning_rate,\n#                                 max_depth=max_depth,\n#                                 subsample=subsample,\n#                                 min_child_weight=min_child_weight,\n#                                 colsample_bytree=colsample_bytree,\n#                                 scale_pos_weight=scale_pos_weight,\n#                                 n_estimators=n_estimators,\n#                                 eval_metric='error'\n#                                   )\n#     grid_search = GridSearchCV(estimator=model_xgb, \n#                                cv=kf,\n#                                param_grid=param_grid,\n#                                scoring='roc_auc'\n#                               )\n#     grid_search.fit(data1, target1)\n\n#     print('模型当前最优参数为:{}'.format(grid_search.best_params_))\n#     print('模型当前最优得分为:{}'.format(grid_search.best_score_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"\"\"通过网格搜索确定最优参数\"\"\"\n# from sklearn.model_selection import GridSearchCV\n\n# def get_best_cv_params(learning_rate=0.1, n_estimators=581, num_leaves=31, max_depth=-1, bagging_fraction=1.0, \n#                        feature_fraction=1.0, bagging_freq=0, min_data_in_leaf=20, min_child_weight=0.001, \n#                        min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=None):\n#     # 设置5折交叉验证\n#     cv_fold = StratifiedKFold(n_splits=5, random_state=0, shuffle=True, )\n    \n#     model_lgb = lgb.LGBMClassifier(learning_rate=learning_rate,\n#                                    n_estimators=n_estimators,\n#                                    num_leaves=num_leaves,\n#                                    max_depth=max_depth,\n#                                    bagging_fraction=bagging_fraction,\n#                                    feature_fraction=feature_fraction,\n#                                    bagging_freq=bagging_freq,\n#                                    min_data_in_leaf=min_data_in_leaf,\n#                                    min_child_weight=min_child_weight,\n#                                    min_split_gain=min_split_gain,\n#                                    reg_lambda=reg_lambda,\n#                                    reg_alpha=reg_alpha,\n#                                    n_jobs= 8\n#                                   )\n#     grid_search = GridSearchCV(estimator=model_lgb, \n#                                cv=cv_fold,\n#                                param_grid=param_grid,\n#                                scoring='roc_auc'\n#                               )\n#     grid_search.fit(X_train, y_train)\n\n#     print('模型当前最优参数为:{}'.format(grid_search.best_params_))\n#     print('模型当前最优得分为:{}'.format(grid_search.best_score_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"\"\"以下代码未运行，耗时较长，请谨慎运行，且每一步的最优参数需要在下一步进行手动更新，请注意\"\"\"\n\n# \"\"\"\n# 需要注意一下的是，除了获取上面的获取num_boost_round时候用的是原生的lightgbm（因为要用自带的cv）\n# 下面配合GridSearchCV时必须使用sklearn接口的lightgbm。\n# \"\"\"\n# \"\"\"设置n_estimators 为581，调整num_leaves和max_depth，这里选择先粗调再细调\"\"\"\n# lgb_params = {'num_leaves': range(10, 80, 5), 'max_depth': range(3,10,2)}\n# get_best_cv_params(learning_rate=0.1, n_estimators=581, num_leaves=None, max_depth=None, min_data_in_leaf=20, \n#                    min_child_weight=0.001,bagging_fraction=1.0, feature_fraction=1.0, bagging_freq=0, \n#                    min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)\n\n# \"\"\"num_leaves为30，max_depth为7，进一步细调num_leaves和max_depth\"\"\"\n# lgb_params = {'num_leaves': range(25, 35, 1), 'max_depth': range(5,9,1)}\n# get_best_cv_params(learning_rate=0.1, n_estimators=85, num_leaves=None, max_depth=None, min_data_in_leaf=20, \n#                    min_child_weight=0.001,bagging_fraction=1.0, feature_fraction=1.0, bagging_freq=0, \n#                    min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)\n\n# \"\"\"\n# 确定min_data_in_leaf为45，min_child_weight为0.001 ，下面进行bagging_fraction、feature_fraction和bagging_freq的调参\n# \"\"\"\n# lgb_params = {'bagging_fraction': [i/10 for i in range(5,10,1)], \n#               'feature_fraction': [i/10 for i in range(5,10,1)],\n#               'bagging_freq': range(0,81,10)\n#              }\n# get_best_cv_params(learning_rate=0.1, n_estimators=85, num_leaves=29, max_depth=7, min_data_in_leaf=45, \n#                    min_child_weight=0.001,bagging_fraction=None, feature_fraction=None, bagging_freq=None, \n#                    min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)\n\n# \"\"\"\n# 确定bagging_fraction为0.4、feature_fraction为0.6、bagging_freq为 ，下面进行reg_lambda、reg_alpha的调参\n# \"\"\"\n# lgb_params = {'reg_lambda': [0,0.001,0.01,0.03,0.08,0.3,0.5], 'reg_alpha': [0,0.001,0.01,0.03,0.08,0.3,0.5]}\n# get_best_cv_params(learning_rate=0.1, n_estimators=85, num_leaves=29, max_depth=7, min_data_in_leaf=45, \n#                    min_child_weight=0.001,bagging_fraction=0.9, feature_fraction=0.9, bagging_freq=40, \n#                    min_split_gain=0, reg_lambda=None, reg_alpha=None, param_grid=lgb_params)\n\n# \"\"\"\n# 确定reg_lambda、reg_alpha都为0，下面进行min_split_gain的调参\n# \"\"\"\n# lgb_params = {'min_split_gain': [i/10 for i in range(0,11,1)]}\n# get_best_cv_params(learning_rate=0.1, n_estimators=85, num_leaves=29, max_depth=7, min_data_in_leaf=45, \n#                    min_child_weight=0.001,bagging_fraction=0.9, feature_fraction=0.9, bagging_freq=40, \n#                    min_split_gain=None, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **模型融合部分**","metadata":{}},{"cell_type":"markdown","source":"本块模型融合部分主要先使用blending算法  融合的基学习器使用逻辑回归（regression）   随机森林(bagging)   xgbregressor (boosting)   三种主要的算法进行合并。   融合的预测模型使用xgbregressor ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2022-03-22T03:51:24.573464Z","iopub.execute_input":"2022-03-22T03:51:24.573782Z","iopub.status.idle":"2022-03-22T03:51:24.57836Z","shell.execute_reply.started":"2022-03-22T03:51:24.573748Z","shell.execute_reply":"2022-03-22T03:51:24.577704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#模型融合中基学习器\nclfs = [LogisticRegression(),\n        RandomForestClassifier(),\n        XGBClassifier()]\n \n#切分一部分数据作为测试集\nX, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=914)\n\n\n#切分训练数据集为d1,d2两部分\nX_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=0.5, random_state=914)\ndataset_d1 = np.zeros((X_d2.shape[0], len(clfs)))\ndataset_d2 = np.zeros((X_predict.shape[0], len(clfs)))\n \nfor j, clf in enumerate(clfs):\n    #依次训练各个单模型\n    clf.fit(X_d1, y_d1)\n    y_submission = clf.predict_proba(X_d2)[:, 1]\n    dataset_d1[:, j] = y_submission\n    #对于测试集，直接用这k个模型的预测值作为新的特征。\n    dataset_d2[:, j] = clf.predict_proba(X_predict)[:, 1]\n    print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_d2[:, j]))\n\n\n#融合使用的模型\nclf = XGBClassifier()\nclf.fit(dataset_d1, y_d2)\ny_submission = clf.predict_proba(dataset_d2)[:, 1]\nprint(\"Val auc Score of Blending: %f\" % (roc_auc_score(y_predict, y_submission)))","metadata":{"execution":{"iopub.status.busy":"2022-03-22T03:59:00.228757Z","iopub.execute_input":"2022-03-22T03:59:00.229139Z","iopub.status.idle":"2022-03-22T04:03:52.921075Z","shell.execute_reply.started":"2022-03-22T03:59:00.229106Z","shell.execute_reply":"2022-03-22T04:03:52.919733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"使用blending 的整体AUC仅仅只有0.7213  甚至不如单XGBCLASSIFIER的表现","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# **尝试加权投票：**","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=4, min_child_weight=2, subsample=0.7,objective='binary:logistic')\n \nvclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('xgb', clf3)], voting='soft', weights=[2, 1, 1])\nvclf = vclf .fit(X_train_split,y_train_split)\n\nprint(\"Val auc Score of weight: %f\" % (roc_auc_score(vclf .predict(X_val), y_val)))\n\n\n############ 加权占比  lf:rf:xgb   2:1:1     AUC=0.686        ","metadata":{"execution":{"iopub.status.busy":"2022-03-22T04:25:30.90089Z","iopub.execute_input":"2022-03-22T04:25:30.901242Z","iopub.status.idle":"2022-03-22T04:37:31.212076Z","shell.execute_reply.started":"2022-03-22T04:25:30.901192Z","shell.execute_reply":"2022-03-22T04:37:31.211373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=4, min_child_weight=2, subsample=0.7,objective='binary:logistic')\n \nvclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('xgb', clf3)], voting='soft', weights=[1, 1, 2])\nvclf = vclf .fit(X_train_split,y_train_split)\n\nprint(\"Val auc Score of weight: %f\" % (roc_auc_score(vclf .predict(X_val), y_val)))\n\n############ 加权占比  lf:rf:xgb   1:1:2    AUC=0.6965","metadata":{"execution":{"iopub.status.busy":"2022-03-22T04:38:51.645293Z","iopub.execute_input":"2022-03-22T04:38:51.645678Z","iopub.status.idle":"2022-03-22T04:50:38.826196Z","shell.execute_reply.started":"2022-03-22T04:38:51.645636Z","shell.execute_reply":"2022-03-22T04:50:38.825131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=4, min_child_weight=2, subsample=0.7,objective='binary:logistic')\n \nvclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('xgb', clf3)], voting='soft', weights=[1, 1, 3])\nvclf = vclf .fit(X_train_split,y_train_split)\n\nprint(\"Val auc Score of weight: %f\" % (roc_auc_score(vclf .predict(X_val), y_val)))\n\n############ 加权占比  lf:rf:xgb   1:1:3    AUC=0.6964","metadata":{"execution":{"iopub.status.busy":"2022-03-22T04:52:00.081603Z","iopub.execute_input":"2022-03-22T04:52:00.081896Z","iopub.status.idle":"2022-03-22T05:03:54.558121Z","shell.execute_reply.started":"2022-03-22T04:52:00.081866Z","shell.execute_reply":"2022-03-22T05:03:54.556406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **尝试Stacking:**","metadata":{}},{"cell_type":"code","source":"# import warnings\n# warnings.filterwarnings('ignore')\n# import itertools\n# import numpy as np\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n# import matplotlib.gridspec as gridspec\n# from sklearn import datasets\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.naive_bayes import GaussianNB \n# from sklearn.ensemble import RandomForestClassifier\n# from mlxtend.classifier import StackingClassifier\n# from sklearn.model_selection import cross_val_score, train_test_split\n# from mlxtend.plotting import plot_learning_curves\n# from mlxtend.plotting import plot_decision_regions\n\n\n\n# clf1 = KNeighborsClassifier(n_neighbors=1)\n# clf2 = RandomForestClassifier(random_state=1)\n# clf3 = GaussianNB()\n# clf4 = LogisticRegression()\n# xgb =  XGBClassifier()\n# sclf = StackingClassifier(classifiers=[clf1, clf2, clf3,clf4], \n#                           meta_classifier=xgb)\n\n\n# label = ['KNN', 'Random Forest', 'Naive Bayes','LG', 'Stacking Classifier']\n# clf_list = [clf1, clf2, clf3,clf4, sclf]\n    \n# fig = plt.figure(figsize=(10,8))\n# gs = gridspec.GridSpec(2, 2)\n# grid = itertools.product([0,1],repeat=2)\n\n\n# clf_cv_mean = []\n# clf_cv_std = []\n# for clf, label, grd in zip(clf_list, label, grid):\n        \n#     scores = cross_val_score(clf, data, target, cv=5, scoring='roc_auc')\n#     print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n#     clf_cv_mean.append(scores.mean())\n#     clf_cv_std.append(scores.std())\n        \n#     clf.fit(data, target)\n#     ax = plt.subplot(gs[grd[0], grd[1]])\n#     fig = plot_decision_regions(X=data, y=target, clf=clf)\n#     plt.title(label)\n\n\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T05:45:16.761149Z","iopub.execute_input":"2022-03-22T05:45:16.761531Z","iopub.status.idle":"2022-03-22T05:45:16.767391Z","shell.execute_reply.started":"2022-03-22T05:45:16.761494Z","shell.execute_reply":"2022-03-22T05:45:16.766523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport itertools\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom sklearn import datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom mlxtend.plotting import plot_learning_curves\nfrom mlxtend.plotting import plot_decision_regions\n\n\n\nclf1 = KNeighborsClassifier(n_neighbors=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\nclf4 = LogisticRegression()\nxgb =  XGBClassifier()\nsclf = StackingClassifier(classifiers=[clf1, clf2, clf3,clf4], \n                          meta_classifier=xgb)\n\n\nscores = cross_val_score(sclf, data, target, cv=5, scoring='roc_auc')\nprint(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n\n\n# label = ['KNN', 'Random Forest', 'Naive Bayes','LG', 'Stacking Classifier']\n# clf_list = [clf1, clf2, clf3,clf4, sclf]\n    \n# fig = plt.figure(figsize=(10,8))\n# gs = gridspec.GridSpec(2, 2)\n# grid = itertools.product([0,1],repeat=2)\n\n\n# clf_cv_mean = []\n# clf_cv_std = []\n# for clf, label, grd in zip(clf_list, label, grid):\n        \n#     scores = cross_val_score(clf, data, target, cv=5, scoring='roc_auc')\n#     print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n#     clf_cv_mean.append(scores.mean())\n#     clf_cv_std.append(scores.std())\n        \n#     clf.fit(data, target)\n#     ax = plt.subplot(gs[grd[0], grd[1]])\n#     fig = plot_decision_regions(X=data, y=target, clf=clf)\n#     plt.title(label)\n\n\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T08:56:14.447775Z","iopub.execute_input":"2022-03-22T08:56:14.448233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model finding for lgb xgb","metadata":{}},{"cell_type":"code","source":"import joblib\n\nlgb_model= joblib.load('../input/model-save/lgb_default.pkl')\n\nxgb_model=joblib.load('../input/model-save/xgb_default.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-03-29T03:41:21.5322Z","iopub.execute_input":"2022-03-29T03:41:21.532593Z","iopub.status.idle":"2022-03-29T03:41:21.637247Z","shell.execute_reply.started":"2022-03-29T03:41:21.532553Z","shell.execute_reply":"2022-03-29T03:41:21.6364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(12,6))\n# lgb.plot_importance(lgb_model, max_num_features=30)\n# plt.title(\"Featurertances\")\n# plt.show()\n\n\n\n\nfig,ax = plt.subplots(figsize=(12,6))\nlgb.plot_importance(lgb_model,\n                ax=ax,\n                max_num_features=30)\n\nplt.title(\"Featurertances\")\nplt.show()\n\n\n\nplt.savefig('./lgb_importance.png')","metadata":{"execution":{"iopub.status.busy":"2022-03-28T16:52:29.355986Z","iopub.execute_input":"2022-03-28T16:52:29.356891Z","iopub.status.idle":"2022-03-28T16:52:29.825315Z","shell.execute_reply.started":"2022-03-28T16:52:29.356846Z","shell.execute_reply":"2022-03-28T16:52:29.823676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.savefig('./lgb_importance.png')","metadata":{"execution":{"iopub.status.busy":"2022-03-28T16:42:03.237111Z","iopub.execute_input":"2022-03-28T16:42:03.237675Z","iopub.status.idle":"2022-03-28T16:42:03.256923Z","shell.execute_reply.started":"2022-03-28T16:42:03.237571Z","shell.execute_reply":"2022-03-28T16:42:03.25625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# booster=xgb_model.booster\n# importance = booster.feature_importance(importance_type='split')\n# feature_name = booster.feature_name()\n\n# feature_importance = pd.DataFrame({'feature_name':feature_name,'importance':importance} )\n# feature_importance.sort_values('importance',ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T16:48:18.970607Z","iopub.execute_input":"2022-03-28T16:48:18.970954Z","iopub.status.idle":"2022-03-28T16:48:18.997363Z","shell.execute_reply.started":"2022-03-28T16:48:18.970925Z","shell.execute_reply":"2022-03-28T16:48:18.996167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(12,6))\nplot_importance(xgb_model,\n                ax=ax,\n                max_num_features=30)\n\nplt.title(\"Featurertances\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T16:57:17.541663Z","iopub.execute_input":"2022-03-28T16:57:17.542578Z","iopub.status.idle":"2022-03-28T16:57:18.078013Z","shell.execute_reply.started":"2022-03-28T16:57:17.542533Z","shell.execute_reply":"2022-03-28T16:57:18.075966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_split, X_val, y_train_split, y_val = train_test_split(data, target, test_size=0.2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import *\n\npre_y = xgb_model.predict(X_val)\nfpr_Nb, tpr_Nb, _ = roc_curve(y_val, pre_y)\naucval = auc(fpr_Nb, tpr_Nb)    # 计算auc的取值\nplt.figure(figsize=(10,8))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_Nb, tpr_Nb,\"r\",linewidth = 3)\nplt.grid()\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"Ture Positive Rate\")\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.title(\"xgb_ROC curve\")\nplt.text(0.15,0.9,\"AUC = \"+str(round(aucval,4)))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T03:50:42.729599Z","iopub.execute_input":"2022-03-29T03:50:42.72992Z","iopub.status.idle":"2022-03-29T03:50:43.536614Z","shell.execute_reply.started":"2022-03-29T03:50:42.72987Z","shell.execute_reply":"2022-03-29T03:50:43.535615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_y = lgb_model.predict(X_val)\nfpr_Nb, tpr_Nb, _ = roc_curve(y_val, pre_y)\naucval = auc(fpr_Nb, tpr_Nb)    # 计算auc的取值\nplt.figure(figsize=(10,8))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_Nb, tpr_Nb,\"r\",linewidth = 3)\nplt.grid()\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"Ture Positive Rate\")\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.title(\"lgb_ROC curve\")\nplt.text(0.15,0.9,\"AUC = \"+str(round(aucval,4)))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T03:50:53.885718Z","iopub.execute_input":"2022-03-29T03:50:53.886034Z","iopub.status.idle":"2022-03-29T03:50:55.400766Z","shell.execute_reply.started":"2022-03-29T03:50:53.886004Z","shell.execute_reply":"2022-03-29T03:50:55.39979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}